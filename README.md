# ğŸ§¬ Histopathology Foundation Models

This repository is inspired by the excellent review by [Bilal et al. (2025)](https://arxiv.org/abs/2502.08333) on foundation models in computational pathology. It aims to be your **one-stop resource** for understanding and implementing foundation models in histopathology.

Whether you're a **student**, **researcher**, or **practitioner**, this repo will help you:

- âœ… Understand the **core concepts** behind foundation models for medical imaging.  
- ğŸ” Access **modular PyTorch implementations** of key models.  
- ğŸ““ Explore **real-life examples and notebooks** that demonstrate practical usage.  
- ğŸŒ Discover **beyond-histopathology applications** in broader medical vision tasks.

> ğŸ”– *"I wish I had this repo when I started my internship. It wouldâ€™ve saved me hours of searching, testing, and debugging. So I created what I wish I had."* â€” **Hana FEKI**

---
## Required Knowledge

In this table, techniques are grouped by research category and ordered by their importance in recent research.

| Category                | Technique     | Paper / Link                                                        | Description                                                                                           | My Explanation / Implementation                   |
|-------------------------|---------------------------|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------|
| **Architectures**       | ViT                       | [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)                           | Vision Transformer architecture for image classification                                        | [vit](./implementations/vit)                       |
| **[Learning Techniques](Learning%20Techniques/)** | Self-Supervised Learning       | [A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends](https://arxiv.org/abs/2301.05712)                 | Learning useful representations from unlabeled data without manual annotations                     | [ssl](./explanations/ssl)                         |
|                         | Semi-Supervised Learning   | [A Survey on Deep Semi-supervised Learning](https://arxiv.org/abs/2103.00550)                  | Learning from small labeled and large unlabeled datasets                                           | [semisupervised](./explanations/semisupervised)  |
|                         | Weakly Supervised Learning | [A Critical Look at Weakly Supervised Learning](https://arxiv.org/abs/2305.17442)              | Learning from weak/noisy labels instead of fully annotated data                                   | [weakly_supervised](./explanations/weakly_supervised) |
|                         | Unsupervised Learning      | [Semi-Supervised and Unsupervised Deep Visual Learning: A Survey](https://arxiv.org/abs/2208.11296) | Learning patterns from unlabeled data                                                             | [unsupervised](./explanations/unsupervised)       |
| **Contrastive Learning** | SimCLR                    | [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)                         | Contrastive SSL framework using augmented views and a simple architecture                         | [simclr](./implementations/simclr)                |
|                         | MoCo                      | [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)                           | Momentum contrast for building dynamic dictionaries                                              | [moco](./implementations/moco)                     |
|                         | CLIP                      | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)                           | Contrastive learning with image-text pairs for multi-modal learning                              | [clip](./implementations/clip)                     |
| **Masked Modeling**     | MAE                       | [MAE](https://arxiv.org/abs/2111.06377)                            | Masked Autoencoder predicting masked image patches                                               | [mae](./implementations/mae)                       |
|                         | MIM                       | [Masked Image Modeling (MIM)](https://arxiv.org/abs/2302.09841)   | Predict masked parts of images during training                                                   | [mim](./implementations/mim)                       |
| **Self-Distillation**   | DINO                      | [DINO](https://arxiv.org/abs/2104.14294)                          | Self-distillation without labels, training student to match teacher                              | [dino](./implementations/dino)                     |
|                         | DinoV2                    | [DinoV2](https://arxiv.org/abs/2304.07193)                        | Improved DINO with stronger recipes                                                              | [dinov2](./implementations/dinov2)                 |
| **Aggregation Methods** | Aggregation               | â€”                                                                  | Techniques to aggregate patch or token embeddings to form global representations                | [aggregation](./explanations/aggregation)          |


  


---
## DINO-based & SSL Advances

This section is inspired by the comprehensive review by [Bilal et al., 2025](https://arxiv.org/abs/2502.08333).  
It summarizes recent **self-supervised learning (SSL)** and **DINO-based** models for whole slide image (WSI) analysis in computational pathology.

| Model          | Paper / Link                                                                 | Official Link                                  | My Implementation & Explanation           | Key Innovation / Feature                                                                                  | Slides   | WSI  | Cell | Gen. | VQA | RptGen | WSI-Lang | Multi-Organ Rpt | MultiModal Rpt | AI Asst | Captioning | Generative |
|----------------|------------------------------------------------------------------------------|-----------------------------------------------|--------------------------------------------|-----------------------------------------------------------------------------------------------------------|----------|------|------|------|-----|--------|----------|-----------------|----------------|---------|------------|------------|
| Virchow       | [arXiv:2309.07778](https://arxiv.org/pdf/2309.07778)                        | ğŸ¤— [paige-ai/Virchow](https://huggingface.co/paige-ai/Virchow) | [models/virchow](./models/virchow)        | Learns using global and local crops with morph-preserving data augmentation                                | 1,488,550 | âŒ   | âŒ   | 17   | âœ…  | âŒ     | âœ…       | âœ…              | âŒ             | âœ…      | âœ…         | âŒ         |
| Virchow2      | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | ğŸ™ [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | [models/virchow2](./models/virchow2)      | Larger dataset with pathology-inspired augmentations to boost training diversity                         | 3,134,922 | âŒ   | âŒ   | 25   | âœ…  | Limited | âœ…       | 37              | âœ…             | âœ…      | âœ…         | âŒ         |
| Virchow2G     | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | ğŸ™ [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | â€”                                          | Scaled up data and model size; mixed magnifications improve generalization                               | 3,134,922 | âŒ   | âŒ   | 25   | âœ…  | Limited | âœ…       | 37              | âœ…             | âœ…      | âœ…         | âŒ         |
| OmniScreen    | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | ğŸ™ [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | â€”                                          | Builds on Virchow2 embeddings for weakly-supervised learning on MSK-IMPACT dataset                         | 30,511    | âŒ   | âŒ   | 27   | âœ…  | âŒ     | âœ…       | 70              | âœ…             | âŒ      | âŒ         | âŒ         |
| H-Optimus-0  | â€”                                                                            | â€”                                             | â€”                                          | Powerful ViT-G with 40 transformer blocks for efficient high-dimensional feature handling                | 500,000   | âŒ   | âŒ   | 32   | âœ…  | âŒ     | âœ…       | 11              | âœ…             | âœ…      | âŒ         | âŒ         |
| Kaiko-ai     | â€”                                                                            | â€”                                             | â€”                                          | Dynamic patch extraction saves GPU; trained on multi-scale TCGA slides                                   | 29,000    | âŒ   | âŒ   | 32   | âœ…  | âŒ     | âœ…       | 8               | âœ…             | âœ…      | âœ…         | âŒ         |
| UNI          | â€”                                                                            | â€”                                             | â€”                                          | Combines masked image modeling and self-distillation with robust regularization                           | 100,000   | âŒ   | âŒ   | 20   | âœ…  | âŒ     | âœ…       | 34              | âœ…             | âœ…      | âœ…         | âŒ         |
| BROW         | â€”                                                                            | â€”                                             | â€”                                          | Adds patch shuffling and color augmentations on top of DINO framework                                   | 11,206    | âŒ   | âŒ   | 6    | âœ…  | âŒ     | âœ…       | 11              | âœ…             | âœ…      | âœ…         | âŒ         |
| Phikon       | [arXiv:2311.11023](https://arxiv.org/abs/2311.11023)                        | â€”                                             | [models/phikon](./models/phikon)           | Uses iBOT self-distillation with masking; robust to image changes                                       | 6,093     | âŒ   | âŒ   | 16   | âœ…  | âŒ     | âœ…       | 17              | âœ…             | âœ…      | âœ…         | âŒ         |
| Phikon-v2    | [arXiv:2311.11023](https://arxiv.org/abs/2311.11023)                        | â€”                                             | â€”                                          | Scaled ViT-L model trained on 460M tiles; strong ensemble for biomarker prediction                       | 58,359    | âŒ   | âŒ   | 30   | âœ…  | Limited | âœ…       | 8               | âœ…             | âœ…      | âŒ         | âŒ         |
| TissueConcepts| â€”                                                                            | â€”                                             | â€”                                          | Joint transformer + convnet encoder trained jointly for classification, segmentation, and detection     | 7,042     | âŒ   | âŒ   | 14   | âœ…  | âŒ     | âœ…       | 16              | âœ…             | âœ…      | âœ…         | âœ…         |
| HIBOU        | [arXiv:2406.06589](https://arxiv.org/abs/2406.06589)                        | â€”                                             | â€”                                          | Trained on 1M+ WSIs with stain augmentation specialized for WSI data                                    | 1,141,581 | âŒ   | âŒ   | 12   | âœ…  | Limited | âœ…       | 12              | âœ…             | âŒ      | âœ…         | âŒ         |
| PLUTO        | [arXiv:2403.00827](https://arxiv.org/abs/2403.00827)                        | â€”                                             | [models/pluto](./models/pluto)             | Multi-scale patching with masked autoencoding and Fourier loss improves out-of-distribution robustness  | 158,852   | âŒ   | âŒ   | 28   | âœ…  | âœ…     | âœ…       | 13              | âœ…             | âœ…      | âœ…         | âŒ         |
| Madeleine    | â€”                                                                            | â€”                                             | â€”                                          | Aligns patches across stains using attention and graph optimal transport for better local alignment    | 16,281    | âŒ   | âŒ   | 1    | âŒ  | âœ…     | âœ…       | 21              | âœ…             | âŒ      | âœ…         | âŒ         |
| PathoDuet    | [arXiv:2403.09677](https://arxiv.org/abs/2403.09677)                        | â€”                                             | [models/pathoduet](./models/pathoduet)     | Custom self-supervised learning using stain and scale augmentations based on MoCoV3                      | 14,896    | âŒ   | âŒ   | 32   | âœ…  | âœ…     | âœ…       | 14              | âœ…             | âŒ      | âœ…         | âŒ         |
| RudolfV      | [arXiv:2403.01821](https://arxiv.org/abs/2403.01821)                        | â€”                                             | â€”                                          | Large-scale training with stain-specific augmentations and pathologist insights                         | 133,998   | âŒ   | âŒ   | 58   | âœ…  | âœ…     | âœ…       | 50              | âœ…             | âœ…      | âœ…         | âŒ         |
| REMEDIS      | [arXiv:2212.08677](https://arxiv.org/abs/2212.08677)                        | ğŸ™ [boschresearch/remedis](https://github.com/boschresearch/remedis) | â€”                                          | Uses SimCLR contrastive learning to improve feature representation                                      | 29,018    | âŒ   | âŒ   | 32   | âŒ  | âœ…     | âœ…       | 15              | âœ…             | âŒ      | âœ…         | âŒ         |
| BEPH         | â€”                                                                            | â€”                                             | â€”                                          | Lightweight BEiT SSL model pretrained with masked image modeling                                       | 11,760    | âŒ   | âŒ   | 32   | âŒ  | âŒ     | âœ…       | 11              | âœ…             | âŒ      | âŒ         | âŒ         |
| COBRA        | [arXiv:2405.20233](https://arxiv.org/abs/2405.20233)                        | ğŸ™ [BatsResearch/COBRA](https://github.com/BatsResearch/COBRA) | â€”                                          | Foundation model for pathology with multi-organ pretraining and multi-task learning                    | 700,000   | âŒ   | âŒ   | 35   | âœ…  | Limited | âœ…       | 25              | âœ…             | âœ…      | âœ…         | âŒ         |


## ğŸ“ Repo Structure

