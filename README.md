# üß¨ Histopathology Foundation Models

This repository is inspired by the excellent review by [Bilal et al. (2025)](https://arxiv.org/abs/2502.08333) on foundation models in computational pathology. It aims to be your **one-stop resource** for understanding and implementing foundation models in histopathology.

Whether you're a **student**, **researcher**, or **practitioner**, this repo will help you:

- ‚úÖ Understand the **core concepts** behind foundation models for medical imaging.  
- üîç Access **modular PyTorch implementations** of key models.  
- üìì Explore **real-life examples and notebooks** that demonstrate practical usage.  
- üåç Discover **beyond-histopathology applications** in broader medical vision tasks.

> üîñ *"I wish I had this repo when I started my internship. It would‚Äôve saved me hours of searching, testing, and debugging. So I created what I wish I had."* ‚Äî **Hana FEKI**

---
## Required Knowledge

In this table, techniques are grouped by research category and ordered by their importance in recent research.

| Category                | Technique     | Paper / Link                                                        | Description                                                                                           | My Explanation / Implementation                   |
|-------------------------|---------------------------|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------|
| **Architectures**       | Transformer                       | [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                           |  Transformer architecture                                        | [transformer](./implementations/transformer)                       |
|                          | ViT                       | [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)                           | Vision Transformer architecture for image classification                                        | [vit](./implementations/vit)                       |
| **[Learning Techniques](Learning%20Techniques/)** | Self-Supervised Learning       | [A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends](https://arxiv.org/abs/2301.05712)                 | Learning useful representations from unlabeled data without manual annotations                     | [ssl](./explanations/ssl)                         |
|                         | Semi-Supervised Learning   | [A Survey on Deep Semi-supervised Learning](https://arxiv.org/abs/2103.00550)                  | Learning from small labeled and large unlabeled datasets                                           | [semisupervised](./explanations/semisupervised)  |
|                         | Weakly Supervised Learning | [A Critical Look at Weakly Supervised Learning](https://arxiv.org/abs/2305.17442)              | Learning from weak/noisy labels instead of fully annotated data                                   | [weakly_supervised](./explanations/weakly_supervised) |
|                         | Unsupervised Learning      | [Semi-Supervised and Unsupervised Deep Visual Learning: A Survey](https://arxiv.org/abs/2208.11296) | Learning patterns from unlabeled data                                                             | [unsupervised](./explanations/unsupervised)       |
| **Contrastive Learning** | SimCLR                    | [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)                         | Contrastive SSL framework using augmented views and a simple architecture                         | [simclr](./implementations/simclr)                |
|                         | MoCo                      | [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)                           | Momentum contrast for building dynamic dictionaries                                              | [moco](./implementations/moco)                     |
|                         | CLIP                      | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)                           | Contrastive learning with image-text pairs for multi-modal learning                              | [clip](./implementations/clip)                     |
| **Masked Modeling**     | MAE                       | [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)                            | Masked Autoencoder predicting masked image patches                                               | [mae](./implementations/mae)                       |
|                         | MIM                       | [Masked Image Modeling: A Survey](https://arxiv.org/abs/2408.06687)   | Predict masked parts of images during training                                                   | [mim](./implementations/mim)                       |
| **Self-Distillation**   | DINO                      | [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)                          | Self-distillation without labels, training student to match teacher                              | [dino](./implementations/dino)                     |
|                         | DinoV2                    | [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)                        | Improved DINO with stronger recipes                                                              | [dinov2](./implementations/dinov2)                 |
| **Aggregation Methods** | Aggregation               | [An Aggregation of Aggregation Methods in Computational Pathology](https://arxiv.org/abs/2211.01256)                                                                  | Techniques to aggregate patch or token embeddings to form global representations                | [aggregation](./explanations/aggregation)          |
| **MOE** | Mixture of Experts               | [MoE]()                                                                  | Techniques to aggregate patch or token embeddings to form global representations                | [aggregation](./explanations/moe)          |
  


---
## DINO-based & SSL Advances

This section is inspired by the comprehensive review by [Bilal et al., 2025](https://arxiv.org/abs/2502.08333).  
It summarizes recent **self-supervised learning (SSL)** and **DINO-based** models for whole slide image (WSI) analysis in computational pathology.

| Model          | Paper / Link                                                                 | Official Link                                  | My Implementation & Explanation           | Key Innovation / Feature                                                                                  | Slides   | WSI  | Cell | Gen. | VQA | RptGen | WSI-Lang | Multi-Organ Rpt | MultiModal Rpt | AI Asst | Captioning | Generative |
|----------------|------------------------------------------------------------------------------|-----------------------------------------------|--------------------------------------------|-----------------------------------------------------------------------------------------------------------|----------|------|------|------|-----|--------|----------|-----------------|----------------|---------|------------|------------|
| Virchow       | [arXiv:2309.07778](https://arxiv.org/pdf/2309.07778)                        | ü§ó [paige-ai/Virchow](https://huggingface.co/paige-ai/Virchow) | [models/virchow](./models/virchow)        | Learns using global and local crops with morph-preserving data augmentation                                | 1,488,550 | ‚ùå   | ‚ùå   | 17   | ‚úÖ  | ‚ùå     | ‚úÖ       | ‚úÖ              | ‚ùå             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Virchow2      | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | üêô [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | [models/virchow2](./models/virchow2)      | Larger dataset with pathology-inspired augmentations to boost training diversity                         | 3,134,922 | ‚ùå   | ‚ùå   | 25   | ‚úÖ  | Limited | ‚úÖ       | 37              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Virchow2G     | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | üêô [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | ‚Äî                                          | Scaled up data and model size; mixed magnifications improve generalization                               | 3,134,922 | ‚ùå   | ‚ùå   | 25   | ‚úÖ  | Limited | ‚úÖ       | 37              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| H-Optimus-0  | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Powerful ViT-G with 40 transformer blocks for efficient high-dimensional feature handling                | 500,000   | ‚ùå   | ‚ùå   | 32   | ‚úÖ  | ‚ùå     | ‚úÖ       | 11              | ‚úÖ             | ‚úÖ      | ‚ùå         | ‚ùå         |
| UNI          | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Combines masked image modeling and self-distillation with robust regularization                           | 100,000   | ‚ùå   | ‚ùå   | 20   | ‚úÖ  | ‚ùå     | ‚úÖ       | 34              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Phikon       | [arXiv:2311.11023](https://arxiv.org/abs/2311.11023)                        | ‚Äî                                             | [models/phikon](./models/phikon)           | Uses iBOT self-distillation with masking; robust to image changes                                       | 6,093     | ‚ùå   | ‚ùå   | 16   | ‚úÖ  | ‚ùå     | ‚úÖ       | 17              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Phikon-v2    | [arXiv:2311.11023](https://arxiv.org/abs/2311.11023)                        | ‚Äî                                             | ‚Äî                                          | Scaled ViT-L model trained on 460M tiles; strong ensemble for biomarker prediction                       | 58,359    | ‚ùå   | ‚ùå   | 30   | ‚úÖ  | Limited | ‚úÖ       | 8               | ‚úÖ             | ‚úÖ      | ‚ùå         | ‚ùå         |
| PLUTO        | [arXiv:2403.00827](https://arxiv.org/abs/2403.00827)                        | ‚Äî                                             | [models/pluto](./models/pluto)             | Multi-scale patching with masked autoencoding and Fourier loss improves out-of-distribution robustness  | 158,852   | ‚ùå   | ‚ùå   | 28   | ‚úÖ  | ‚úÖ     | ‚úÖ       | 13              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| RudolfV      | [arXiv:2403.01821](https://arxiv.org/abs/2403.01821)                        | ‚Äî                                             | ‚Äî                                          | Large-scale training with stain-specific augmentations and pathologist insights                         | 133,998   | ‚ùå   | ‚ùå   | 58   | ‚úÖ  | ‚úÖ     | ‚úÖ       | 50              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |

## üìÅ Repo Structure

