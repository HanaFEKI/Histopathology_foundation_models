# üß¨ Histopathology Foundation Models

This repository is inspired by the excellent review by [Bilal et al. (2025)](https://arxiv.org/abs/2502.08333) on foundation models in computational pathology. It aims to be your **one-stop resource** for understanding and implementing foundation models in histopathology.

Whether you're a **student**, **researcher**, or **practitioner**, this repo will help you:

- ‚úÖ Understand the **core concepts** behind foundation models for medical imaging.  
- üîç Access **modular PyTorch implementations** of key models.  
- üìì Explore **real-life examples and notebooks** that demonstrate practical usage.  
- üåç Discover **beyond-histopathology applications** in broader medical vision tasks.

> üîñ *"I wish I had this repo when I started my internship. It would‚Äôve saved me hours of searching, testing, and debugging. So I created what I wish I had."* ‚Äî **Hana FEKI**

---
## Required Knowledge

In this table, techniques are grouped by research category and ordered by their importance in recent research.

| Category                | Technique     | Paper / Link                                                        | Description                                                                                           | My Explanation / Implementation                   |
|-------------------------|---------------------------|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|---------------------------------------------------|
| **Architectures**       | ViT                       | [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)                           | Vision Transformer architecture for image classification                                        | [vit](./implementations/vit)                       |
| **[Learning Techniques](Learning%20Techniques/)** | Self-Supervised Learning       | [A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends](https://arxiv.org/abs/2301.05712)                 | Learning useful representations from unlabeled data without manual annotations                     | [ssl](./explanations/ssl)                         |
|                         | Semi-Supervised Learning   | [A Survey on Deep Semi-supervised Learning](https://arxiv.org/abs/2103.00550)                  | Learning from small labeled and large unlabeled datasets                                           | [semisupervised](./explanations/semisupervised)  |
|                         | Weakly Supervised Learning | [A Critical Look at Weakly Supervised Learning](https://arxiv.org/abs/2305.17442)              | Learning from weak/noisy labels instead of fully annotated data                                   | [weakly_supervised](./explanations/weakly_supervised) |
|                         | Unsupervised Learning      | [Semi-Supervised and Unsupervised Deep Visual Learning: A Survey](https://arxiv.org/abs/2208.11296) | Learning patterns from unlabeled data                                                             | [unsupervised](./explanations/unsupervised)       |
| **Contrastive Learning** | SimCLR                    | [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)                         | Contrastive SSL framework using augmented views and a simple architecture                         | [simclr](./implementations/simclr)                |
|                         | MoCo                      | [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)                           | Momentum contrast for building dynamic dictionaries                                              | [moco](./implementations/moco)                     |
|                         | CLIP                      | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)                           | Contrastive learning with image-text pairs for multi-modal learning                              | [clip](./implementations/clip)                     |
| **Masked Modeling**     | MAE                       | [MAE](https://arxiv.org/abs/2111.06377)                            | Masked Autoencoder predicting masked image patches                                               | [mae](./implementations/mae)                       |
|                         | MIM                       | [Masked Image Modeling (MIM)](https://arxiv.org/abs/2302.09841)   | Predict masked parts of images during training                                                   | [mim](./implementations/mim)                       |
| **Self-Distillation**   | DINO                      | [DINO](https://arxiv.org/abs/2104.14294)                          | Self-distillation without labels, training student to match teacher                              | [dino](./implementations/dino)                     |
|                         | DinoV2                    | [DinoV2](https://arxiv.org/abs/2304.07193)                        | Improved DINO with stronger recipes                                                              | [dinov2](./implementations/dinov2)                 |
| **Aggregation Methods** | Aggregation               | ‚Äî                                                                  | Techniques to aggregate patch or token embeddings to form global representations                | [aggregation](./explanations/aggregation)          |


  


---
## DINO-based & SSL Advances

This section is inspired by the comprehensive review by [Bilal et al., 2025](https://arxiv.org/abs/2502.08333).  
It summarizes recent **self-supervised learning (SSL)** and **DINO-based** models for whole slide image (WSI) analysis in computational pathology.

| Model          | Paper / Link                                                                 | Official Link                                  | My Implementation & Explanation           | Key Innovation / Feature                                                                                  | Slides   | WSI  | Cell | Gen. | VQA | RptGen | WSI-Lang | Multi-Organ Rpt | MultiModal Rpt | AI Asst | Captioning | Generative |
|----------------|------------------------------------------------------------------------------|-----------------------------------------------|--------------------------------------------|-----------------------------------------------------------------------------------------------------------|----------|------|------|------|-----|--------|----------|-----------------|----------------|---------|------------|------------|
| Virchow       | [arXiv:2309.07778](https://arxiv.org/pdf/2309.07778)                        | ü§ó [paige-ai/Virchow](https://huggingface.co/paige-ai/Virchow) | [models/virchow](./models/virchow)        | Learns using global and local crops with morph-preserving data augmentation                                | 1,488,550 | ‚ùå   | ‚ùå   | 17   | ‚úÖ  | ‚ùå     | ‚úÖ       | ‚úÖ              | ‚ùå             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Virchow2      | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | üêô [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | [models/virchow2](./models/virchow2)      | Larger dataset with pathology-inspired augmentations to boost training diversity                         | 3,134,922 | ‚ùå   | ‚ùå   | 25   | ‚úÖ  | Limited | ‚úÖ       | 37              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Virchow2G     | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | üêô [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | ‚Äî                                          | Scaled up data and model size; mixed magnifications improve generalization                               | 3,134,922 | ‚ùå   | ‚ùå   | 25   | ‚úÖ  | Limited | ‚úÖ       | 37              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| OmniScreen    | [arXiv:2403.10870](https://arxiv.org/abs/2403.10870)                        | üêô [BatsResearch/Virchow](https://github.com/BatsResearch/Virchow) | ‚Äî                                          | Builds on Virchow2 embeddings for weakly-supervised learning on MSK-IMPACT dataset                         | 30,511    | ‚ùå   | ‚ùå   | 27   | ‚úÖ  | ‚ùå     | ‚úÖ       | 70              | ‚úÖ             | ‚ùå      | ‚ùå         | ‚ùå         |
| H-Optimus-0  | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Powerful ViT-G with 40 transformer blocks for efficient high-dimensional feature handling                | 500,000   | ‚ùå   | ‚ùå   | 32   | ‚úÖ  | ‚ùå     | ‚úÖ       | 11              | ‚úÖ             | ‚úÖ      | ‚ùå         | ‚ùå         |
| Kaiko-ai     | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Dynamic patch extraction saves GPU; trained on multi-scale TCGA slides                                   | 29,000    | ‚ùå   | ‚ùå   | 32   | ‚úÖ  | ‚ùå     | ‚úÖ       | 8               | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| UNI          | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Combines masked image modeling and self-distillation with robust regularization                           | 100,000   | ‚ùå   | ‚ùå   | 20   | ‚úÖ  | ‚ùå     | ‚úÖ       | 34              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| BROW         | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Adds patch shuffling and color augmentations on top of DINO framework                                   | 11,206    | ‚ùå   | ‚ùå   | 6    | ‚úÖ  | ‚ùå     | ‚úÖ       | 11              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Phikon       | [arXiv:2311.11023](https://arxiv.org/abs/2311.11023)                        | ‚Äî                                             | [models/phikon](./models/phikon)           | Uses iBOT self-distillation with masking; robust to image changes                                       | 6,093     | ‚ùå   | ‚ùå   | 16   | ‚úÖ  | ‚ùå     | ‚úÖ       | 17              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Phikon-v2    | [arXiv:2311.11023](https://arxiv.org/abs/2311.11023)                        | ‚Äî                                             | ‚Äî                                          | Scaled ViT-L model trained on 460M tiles; strong ensemble for biomarker prediction                       | 58,359    | ‚ùå   | ‚ùå   | 30   | ‚úÖ  | Limited | ‚úÖ       | 8               | ‚úÖ             | ‚úÖ      | ‚ùå         | ‚ùå         |
| TissueConcepts| ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Joint transformer + convnet encoder trained jointly for classification, segmentation, and detection     | 7,042     | ‚ùå   | ‚ùå   | 14   | ‚úÖ  | ‚ùå     | ‚úÖ       | 16              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚úÖ         |
| HIBOU        | [arXiv:2406.06589](https://arxiv.org/abs/2406.06589)                        | ‚Äî                                             | ‚Äî                                          | Trained on 1M+ WSIs with stain augmentation specialized for WSI data                                    | 1,141,581 | ‚ùå   | ‚ùå   | 12   | ‚úÖ  | Limited | ‚úÖ       | 12              | ‚úÖ             | ‚ùå      | ‚úÖ         | ‚ùå         |
| PLUTO        | [arXiv:2403.00827](https://arxiv.org/abs/2403.00827)                        | ‚Äî                                             | [models/pluto](./models/pluto)             | Multi-scale patching with masked autoencoding and Fourier loss improves out-of-distribution robustness  | 158,852   | ‚ùå   | ‚ùå   | 28   | ‚úÖ  | ‚úÖ     | ‚úÖ       | 13              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| Madeleine    | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Aligns patches across stains using attention and graph optimal transport for better local alignment    | 16,281    | ‚ùå   | ‚ùå   | 1    | ‚ùå  | ‚úÖ     | ‚úÖ       | 21              | ‚úÖ             | ‚ùå      | ‚úÖ         | ‚ùå         |
| PathoDuet    | [arXiv:2403.09677](https://arxiv.org/abs/2403.09677)                        | ‚Äî                                             | [models/pathoduet](./models/pathoduet)     | Custom self-supervised learning using stain and scale augmentations based on MoCoV3                      | 14,896    | ‚ùå   | ‚ùå   | 32   | ‚úÖ  | ‚úÖ     | ‚úÖ       | 14              | ‚úÖ             | ‚ùå      | ‚úÖ         | ‚ùå         |
| RudolfV      | [arXiv:2403.01821](https://arxiv.org/abs/2403.01821)                        | ‚Äî                                             | ‚Äî                                          | Large-scale training with stain-specific augmentations and pathologist insights                         | 133,998   | ‚ùå   | ‚ùå   | 58   | ‚úÖ  | ‚úÖ     | ‚úÖ       | 50              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |
| REMEDIS      | [arXiv:2212.08677](https://arxiv.org/abs/2212.08677)                        | üêô [boschresearch/remedis](https://github.com/boschresearch/remedis) | ‚Äî                                          | Uses SimCLR contrastive learning to improve feature representation                                      | 29,018    | ‚ùå   | ‚ùå   | 32   | ‚ùå  | ‚úÖ     | ‚úÖ       | 15              | ‚úÖ             | ‚ùå      | ‚úÖ         | ‚ùå         |
| BEPH         | ‚Äî                                                                            | ‚Äî                                             | ‚Äî                                          | Lightweight BEiT SSL model pretrained with masked image modeling                                       | 11,760    | ‚ùå   | ‚ùå   | 32   | ‚ùå  | ‚ùå     | ‚úÖ       | 11              | ‚úÖ             | ‚ùå      | ‚ùå         | ‚ùå         |
| COBRA        | [arXiv:2405.20233](https://arxiv.org/abs/2405.20233)                        | üêô [BatsResearch/COBRA](https://github.com/BatsResearch/COBRA) | ‚Äî                                          | Foundation model for pathology with multi-organ pretraining and multi-task learning                    | 700,000   | ‚ùå   | ‚ùå   | 35   | ‚úÖ  | Limited | ‚úÖ       | 25              | ‚úÖ             | ‚úÖ      | ‚úÖ         | ‚ùå         |


## üìÅ Repo Structure

