# Aggregation Techniques in Computational Pathology

Aggregation is a fundamental component in computational pathology models, particularly in **Multiple Instance Learning (MIL)** settings. It refers to the method used to combine patch-level or token-level features into a single global slide-level representation. The choice of aggregation can significantly impact model performance, especially for whole slide images (WSIs), which are extremely large and often processed in smaller patches.

## ğŸ“Œ Why Aggregation Matters

In digital pathology, we usually do **not** have labels for individual patches â€” only for the **entire slide** (e.g., tumor vs. normal). Therefore, aggregation functions are essential to convert a set of patch embeddings (instances) into a **single vector** suitable for downstream tasks like classification or report generation.

Aggregation helps:
- Reduce computational burden
- Preserve global context from localized features
- Enable end-to-end learning from weak labels (WSI-level)

## ğŸ§  Aggregation Techniques

| **Method** | **Learnable** | **Description** | **Pros** | **Cons** | **Example Models** |
|------------|---------------|-----------------|----------|----------|---------------------|
| **Mean Pooling (Avg MIL)** | âŒ | Computes the **average** of all patch embeddings to represent the whole slide. Simple and effective. | âœ… Fast and parameter-free<br>âœ… Easy to implement | âŒ Treats all patches equally<br>âŒ May dilute critical features | ViT (baseline), CLAM (avg variant) |
| **Max Pooling** | âŒ | Selects the **most activated patch** (highest value) to represent the slide. Assumes one key patch determines the label. | âœ… Highlights relevant regions<br>âœ… No training needed | âŒ Ignores all other patches<br>âŒ Highly sensitive to noise | DeepMIL |
| **Attention-based MIL** | âœ… | From [Ilse et al., 2018](https://arxiv.org/abs/1802.04712). Learns attention weights to focus on informative patches. Aggregates a weighted sum of patches. <br><br>**Formula:**<br>\( z = \sum_{i=1}^{n} \alpha_i h_i \), where:<br>\( \alpha_i = \frac{\exp(w^T \tanh(V h_i^T))}{\sum_j \exp(w^T \tanh(V h_j^T))} \) | âœ… Learns patch importance<br>âœ… Interpretable<br>âœ… Works well in WSI settings | âŒ May overfit on small data<br>âŒ Adds parameters | CLAM, TransMIL, iBOT |
| **Gated Attention MIL** | âœ… | A variant of attention pooling that uses both **tanh** and **sigmoid** nonlinearities to refine attention signals. | âœ… More expressive<br>âœ… Better patch discrimination | âŒ Requires more compute<br>âŒ Slightly complex | CLAM (gated), ABCMIL |
| **Transformer Aggregation** | âœ… | Uses **transformer self-attention** to model relationships between patches and produce a global token or aggregate feature. | âœ… Captures rich interactions<br>âœ… Scales well with data<br>âœ… Powerful for WSI classification | âŒ Memory intensive<br>âŒ Longer training time | TransMIL, UNI, UNIv2, RudolfV |
| **Learnable Pooling (NetVLAD, DeepSets, etc.)** | âœ… | Advanced aggregation techniques for unordered inputs. NetVLAD uses soft assignments to learned cluster centers. DeepSets use permutation-invariant architectures. | âœ… Models global structure<br>âœ… Flexible | âŒ Less interpretable<br>âŒ Heavier to tune | PathFormer, retrieval models |

## ğŸ” Detailed Formula Explanation (Attention-based MIL)

From Ilse et al., 2018:

``` math
z = \sum_{i=1}^{n} \alpha_i h_i \quad \text{where } \alpha_i = \frac{\exp(w^T \tanh(Vh_i^T))}{\sum_j \exp(w^T \tanh(Vh_j^T))}
```

| Variable | Meaning |
|--------|---------|
| \( h_i \in \mathbb{R}^d \) | Embedding of patch \(i\) |
| \( z \in \mathbb{R}^d \) | Aggregated slide-level embedding |
| \( V \in \mathbb{R}^{l \times d} \) | Weight matrix for attention |
| \( w \in \mathbb{R}^l \) | Weight vector to score each patch |
| \( \tanh \) | Activation function (nonlinearity) |
| \( \alpha_i \) | Importance score of patch \(i\), normalized with softmax |


## ğŸ“‚ References

- Ilse et al., 2018 â€” [Attention-based Deep MIL](https://arxiv.org/abs/1802.04712)
- Lu et al., 2021 â€” CLAM ([code](https://github.com/mahmoodlab/CLAM))
- Shao et al., 2021 â€” TransMIL ([paper](https://arxiv.org/abs/2106.00908))
- UNI/UNIv2 â€” [Virchow Repo](https://github.com/BatsResearch/Virchow)
- ABCMIL, iBOT â€” see relevant GitHub implementations

## Implementation

You can find implementations or scripts for several of these aggregation types in:
``` bash
./explanations/aggregation/mean_pooling.py
./explanations/aggregation/attention_pooling.py
./explanations/aggregation/gated_attention.py
./explanations/aggregation/transformer_agg.py
```

> ğŸ› ï¸ Feel free to extend this by plugging in your models and observing which aggregation style best suits your pathology tasks!
